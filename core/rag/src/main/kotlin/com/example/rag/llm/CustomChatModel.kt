package com.example.rag.llm

import android.llama.cpp.LLamaAndroid
import android.util.Log
import com.example.rag.TAG
import dev.langchain4j.data.message.AiMessage
import dev.langchain4j.data.message.ChatMessage
import dev.langchain4j.model.StreamingResponseHandler
import dev.langchain4j.model.chat.StreamingChatLanguageModel
import dev.langchain4j.model.output.Response
import kotlinx.coroutines.Dispatchers
import kotlinx.coroutines.Job
import kotlinx.coroutines.cancelAndJoin
import kotlinx.coroutines.launch
import kotlinx.coroutines.runBlocking
import javax.inject.Inject
import kotlin.system.measureTimeMillis

class CustomChatModel @Inject constructor(
):
StreamingChatLanguageModel {

    private val llamaAndroid by lazy { LLamaAndroid.instance() }

    private var chatJob: Job? = null

    /**
     * LLM
     * ëª¨ë¸ ê²½ë¡œëŠ” ì„ì˜ë¡œ ì„¤ì •
     *
     * ex) data/local/tmp/llm/ ~
     */
    fun loadGGUFLlmModel(): Boolean = runCatching {
        // ì„ì‹œ ë°©í¸
        runBlocking {
            llamaAndroid.load("data/local/tmp/llm/SmolLM2-135M-Instruct.F16.gguf")
        }
        true
    }.onFailure {
        Log.e(TAG, "loadGGUFLlmModel fatal $it")
    }.onSuccess {
        Log.i(TAG, "loadGGUFLlmModel success")
    }.getOrDefault(false)


    override fun generate(
        messages: MutableList<ChatMessage>,
        handler: StreamingResponseHandler<AiMessage>
    ) {
        if (!llamaAndroid.isInitialized()) {
            loadGGUFLlmModel()
        }

        // TODO: ë‹¤ìŒ LangChain4j ë²„ì „ì—ì„œ ì‚­ì œë˜ëŠ” ì½”ë“œ
        // TODO: ë²„ì „ ë³€ê²½ ì‹œ ê²€í†  í•„ìš”

        val query = messages.joinToString("\n") { it.text() }
        val answer = llamaAndroid.send(query)

        runBlocking {
            chatJob?.cancelAndJoin()
            chatJob = launch(Dispatchers.Default) {
                answer.collect {
                    try {
                        if (it == "<EOS>") {
                            handler.onComplete(Response(AiMessage((query))))
                        } else {
                            handler.onNext(it)
                        }
                    } catch (e: Exception) {
                        handler.onError(e)
                    }
                }
            }
        }
    }

    suspend fun close() {
        chatJob?.cancelAndJoin()
        chatJob = null
        // llamaAndroid.unload() TODO: ì˜¤ë¥˜ ë°œìƒ
    }

    fun classifyQuery(query: String): String {
        if (!llamaAndroid.isInitialized()) {
            loadGGUFLlmModel()
        }
        val prompt = """
        You are an AI assistant. You must classify user questions into two types:
        ğŸ” **"direct"** â†’ Questions that you can answer immediately (e.g., "What is the population of Seoul?")
        ğŸ” **"RAG"** â†’ Questions that require searching user records or documents (e.g., "What channels have I watched the most?")
        ğŸ“Œ Question: "$query"  
        **Output only one word: "RAG" or "direct". No explanations.**
    """.trimIndent()
        val response = StringBuilder()
        runBlocking {
            chatJob?.cancelAndJoin()
            chatJob = launch(Dispatchers.IO) {
                try {
                    llamaAndroid.send(prompt).collect {
                        if (it != "<EOS>") {
                            response.append(it)
                        }
                    }
                } catch (e: Exception) {
                    Log.e(TAG, "ğŸš¨ Llama ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: ${e.message}")
                }
            }
        }
        val result = response.toString().trim()
        return when {
            result.equals("RAG", ignoreCase = true) -> "RAG"
            result.equals("direct", ignoreCase = true) -> "direct"
            else -> {
                Log.w(TAG, "âš  LLMì´ ì˜ˆìƒí•˜ì§€ ì•Šì€ ì‘ë‹µì„ ë°˜í™˜í•¨: $result")
                "direct"
            }
        }
    }

    fun chatSync(query: String): String {
        if (!llamaAndroid.isInitialized()) {
            loadGGUFLlmModel()
        }
        val response = StringBuilder()
        val time = measureTimeMillis {
            runBlocking(Dispatchers.IO) {
                try {
                    llamaAndroid.send(query).collect { token ->
                        if (token != "<EOS>") {
                            response.append(token)
                        }
                    }
                } catch (e: Exception) {
                    Log.e(TAG, "ğŸš¨ Llama execution error in chatSync: ${e.message}")
                }
            }
        }
        Log.i(TAG, "chatSync completed in $time ms")
        return response.toString().trim()
    }

}